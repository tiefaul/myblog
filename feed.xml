<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://www.levitatecold.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.levitatecold.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-05T16:06:09+00:00</updated><id>https://www.levitatecold.com/feed.xml</id><title type="html">Tyler Faulhaber</title><subtitle>Welcome to Tyler&apos;s IT Insights, where I document my journey through the world of technology. From coding projects and homelab experiments to insights on cybersecurity, cloud computing, and more—join me as I explore, learn, and share my experiences in the ever-evolving IT landscape. </subtitle><entry><title type="html">Building an AI Chatbot with Django, HTMX, and Langflow</title><link href="https://www.levitatecold.com/blog/2025/AI-Chatbot/" rel="alternate" type="text/html" title="Building an AI Chatbot with Django, HTMX, and Langflow"/><published>2025-08-05T11:00:00+00:00</published><updated>2025-08-05T11:00:00+00:00</updated><id>https://www.levitatecold.com/blog/2025/AI-Chatbot</id><content type="html" xml:base="https://www.levitatecold.com/blog/2025/AI-Chatbot/"><![CDATA[<p>I wanted to share a project I’ve been working on as part of my experimental Django website — an AI-powered chatbot that responds to questions using a collection of personal notes I’ve fed into it.</p> <p>This app is an exciting combination of several tools and frameworks including:</p> <ul> <li>Django - the foundation of the application</li> <li>Django Channels - for managing real-time WebSocket communication</li> <li>HTMX - to handle dynamic front-end interactions without heavy JavaScript</li> <li>Langflow - for orchestrating and managing the AI workflow</li> </ul> <p>Together, these tools made it possible to build what you see below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/msedge_DQzFZZjXZQ.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <p>Below is a snippet from my consumer.py file, which handles the WebSocket connections between the frontend and the backend:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="n">channels.db</span> <span class="kn">import</span> <span class="n">database_sync_to_async</span>
<span class="kn">from</span> <span class="n">ai_chatbot.models</span> <span class="kn">import</span> <span class="n">AiConversationModel</span>
<span class="kn">from</span> <span class="n">channels.generic.websocket</span> <span class="kn">import</span> <span class="n">AsyncWebsocketConsumer</span>
<span class="kn">from</span> <span class="n">django.template.loader</span> <span class="kn">import</span> <span class="n">render_to_string</span>
<span class="kn">from</span> <span class="n">asgiref.sync</span> <span class="kn">import</span> <span class="n">sync_to_async</span>



<span class="n">BASE_API_URL</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;API URL&gt;</span><span class="sh">"</span> <span class="c1"># Langflow server.
</span><span class="n">FLOW_ID</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">FLOW_ID</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Langflow flow id.
</span><span class="n">URL</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">BASE_API_URL</span><span class="si">}</span><span class="s">/api/v1/run/</span><span class="si">{</span><span class="n">FLOW_ID</span><span class="si">}</span><span class="sh">"</span> <span class="c1"># Endpoint url to run langflow.
</span>
<span class="sh">"""</span><span class="s">
Create consumer for Async Websocket.
</span><span class="sh">"""</span>

<span class="k">class</span> <span class="nc">AiConversationConsumer</span><span class="p">(</span><span class="n">AsyncWebsocketConsumer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Create async connect method.
    </span><span class="sh">"""</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">connect</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Grab id and user information from the websocket scope.
</span>        <span class="n">self</span><span class="p">.</span><span class="nb">id</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scope</span><span class="p">[</span><span class="sh">"</span><span class="s">url_route</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">kwargs</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">user_id</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scope</span><span class="p">[</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">]</span> <span class="c1"># Using self.user makes this variable available to other methods i.e disconnect method.
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">user</span><span class="p">.</span><span class="n">is_authenticated</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">user</span><span class="si">}</span><span class="s"> Connected.</span><span class="sh">"</span><span class="p">)</span>
            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">accept</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

    <span class="sh">"""</span><span class="s">
    Create async disconnect method.
    </span><span class="sh">"""</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">disconnect</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">code</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">user</span><span class="si">}</span><span class="s"> Disconnected. Saving the conversation.</span><span class="sh">"</span><span class="p">)</span>
        <span class="c1"># Save conversation once user disconnects.
</span>        <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">save_conversation</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span> <span class="n">user</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">user</span><span class="p">,</span> <span class="n">new_messages</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">conversation_json_messages</span><span class="p">)</span>
        <span class="k">pass</span>

    <span class="sh">"""</span><span class="s">
    Create async receive method.
    </span><span class="sh">"""</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">receive</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text_data</span><span class="p">):</span>
        <span class="c1"># Turn text_data(json) argument, sent from the client, into a dictionary.
</span>        <span class="n">text_data_json</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">text_data</span><span class="p">)</span> 
        <span class="n">message</span> <span class="o">=</span> <span class="n">text_data_json</span><span class="p">[</span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">]</span> <span class="c1"># Name of your form value.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conversation_json_messages</span> <span class="o">=</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_conversation</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span> <span class="n">user</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">user</span><span class="p">)</span>

        <span class="c1"># Append/save the text_data message to the AiConversationModel.conversation table.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conversation_json_messages</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">User</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">:</span> <span class="n">message</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>

        <span class="c1"># Turn user message into HTML and send to HTMX frontend. render_to_string will take the HTML template, convert it to a string, and send to HTMX. HTMX will then read the string as HTML.
</span>        <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span>
            <span class="n">text_data</span> <span class="o">=</span> <span class="nf">render_to_string</span><span class="p">(</span><span class="n">template_name</span><span class="o">=</span><span class="sh">"</span><span class="s">ai_chatbot/websocket_templates/user_message.html</span><span class="sh">"</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">user</span><span class="p">,</span> <span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">:</span> <span class="n">message</span><span class="p">})</span>
        <span class="p">)</span>

        <span class="sh">"""</span><span class="s">
        Create AI message.
        </span><span class="sh">"""</span>
    
        <span class="n">ai_message</span> <span class="o">=</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">run_flow</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">URL</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">langflow_api_key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">LANGFLOW_API_KEY</span><span class="sh">"</span><span class="p">))</span>
        <span class="n">ai_parsed_message</span> <span class="o">=</span> <span class="n">ai_message</span><span class="p">[</span><span class="sh">"</span><span class="s">outputs</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">outputs</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">results</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]</span>
        
        <span class="c1"># Sanitize the HTML.
</span>        <span class="kn">from</span> <span class="n">html_sanitizer</span> <span class="kn">import</span> <span class="n">Sanitizer</span>
        <span class="n">sanitized_ai_parsed_message</span> <span class="o">=</span> <span class="nc">Sanitizer</span><span class="p">().</span><span class="nf">sanitize</span><span class="p">(</span><span class="n">ai_parsed_message</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">conversation_json_messages</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Ai</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">:</span> <span class="n">sanitized_ai_parsed_message</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>

        <span class="c1"># Turn Ai markdown message into html.
</span>        <span class="kn">import</span> <span class="n">markdown</span>
        <span class="n">ai_html_message</span> <span class="o">=</span> <span class="n">markdown</span><span class="p">.</span><span class="nf">markdown</span><span class="p">(</span><span class="n">ai_parsed_message</span><span class="p">)</span>

        <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span>
            <span class="n">text_data</span> <span class="o">=</span> <span class="nf">render_to_string</span><span class="p">(</span><span class="n">template_name</span><span class="o">=</span><span class="sh">"</span><span class="s">ai_chatbot/websocket_templates/ai_message.html</span><span class="sh">"</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">ai_html_message</span><span class="sh">"</span><span class="p">:</span> <span class="n">ai_html_message</span><span class="p">})</span>
        <span class="p">)</span>

        <span class="c1">#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#
</span>        <span class="c1"># Below is an alternative way to send the message not using the `render_to_string` method.
</span>        <span class="c1"># Essentially you are creating the html template and storing it in a variable 
</span>        <span class="c1"># and then sending it off to be received as json to the frontend HTMX
</span>
        <span class="c1"># message_html = f"&lt;div hx-swap-oob='beforeend:#ai-messages'&gt;&lt;b&gt;ChatBot:&lt;/b&gt;&lt;p&gt;{ai_html_message}&lt;/p&gt;&lt;/div&gt;" # Be careful with your quotation marks. Inner HTML must be in single quotes.
</span>        <span class="c1"># await self.send(
</span>        <span class="c1">#     text_data = json.dumps(
</span>        <span class="c1">#         {
</span>        <span class="c1">#             "message": message_html,
</span>        <span class="c1">#         }
</span>        <span class="c1">#     )
</span>        <span class="c1"># )
</span>        <span class="c1">#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#
</span>
        <span class="c1"># ------------------------------------------------------------------------------------------------------------ #
</span>        <span class="c1"># Below are notes on using a channel layer. Channel layers can be used to group
</span>        <span class="c1"># other consumer instances to talk with each other by using the channel_layer.group_add function 
</span>
        <span class="c1"># Type key calls a function (chat.message = chat_message) and passes it to the event argument of the function.
</span>        <span class="c1"># await self.channel_layer.send(
</span>        <span class="c1">#     self.channel_name,
</span>        <span class="c1">#     {
</span>        <span class="c1">#         "type": "chat.message",
</span>        <span class="c1">#         "message": message,
</span>        <span class="c1">#         "username": self.user
</span>        <span class="c1">#     }
</span>        <span class="c1"># )
</span>        <span class="c1"># ------------------------------------------------------------------------------------------------------------ #
</span>
    <span class="sh">"""</span><span class="s">
    Create a sync_to_async function that takes an api call to your langflow api server.
    </span><span class="sh">"""</span>

    <span class="nd">@sync_to_async</span>
    <span class="k">def</span> <span class="nf">run_flow</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">output_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">chat</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># Defaults to chat.
</span>        <span class="n">input_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">chat</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">langflow_api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">):</span> <span class="c1"># This all should output a dictionary (response.json()).
</span>        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Supply json payload for api call.
</span>            <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
                <span class="sh">"</span><span class="s">input_value</span><span class="sh">"</span><span class="p">:</span> <span class="n">message</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">output_type</span><span class="sh">"</span><span class="p">:</span> <span class="n">output_type</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">input_type</span><span class="sh">"</span><span class="p">:</span> <span class="n">input_type</span>
            <span class="p">}</span>
            <span class="c1"># Supply headers for api call.
</span>            <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
                <span class="sh">"</span><span class="s">Content-Type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">application/json</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">x-api-key</span><span class="sh">"</span><span class="p">:</span> <span class="n">langflow_api_key</span>
            <span class="p">}</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()</span>
        
        <span class="k">except</span> <span class="n">requests</span><span class="p">.</span><span class="n">JSONDecodeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error with json: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>


    <span class="sh">"""</span><span class="s">
    Create save and get conversation methods.
    </span><span class="sh">"""</span>

    <span class="nd">@database_sync_to_async</span>
    <span class="k">def</span> <span class="nf">get_conversation</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">user</span><span class="p">):</span>
        <span class="n">ai_conversation</span> <span class="o">=</span> <span class="n">AiConversationModel</span><span class="p">.</span><span class="n">objects</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="nb">id</span><span class="p">,</span> <span class="n">user</span><span class="o">=</span><span class="n">user</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ai_conversation</span><span class="p">.</span><span class="n">conversation</span> <span class="k">if</span> <span class="n">ai_conversation</span><span class="p">.</span><span class="n">conversation</span> <span class="k">else</span> <span class="p">[]</span>
    
    <span class="nd">@database_sync_to_async</span>
    <span class="k">def</span> <span class="nf">save_conversation</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">user</span><span class="p">,</span> <span class="n">new_messages</span><span class="p">):</span>
        <span class="n">ai_conversation</span> <span class="o">=</span> <span class="n">AiConversationModel</span><span class="p">.</span><span class="n">objects</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="nb">id</span><span class="p">,</span> <span class="n">user</span><span class="o">=</span><span class="n">user</span><span class="p">)</span>
        <span class="n">ai_conversation</span><span class="p">.</span><span class="n">conversation</span> <span class="o">=</span> <span class="n">new_messages</span>
        <span class="n">ai_conversation</span><span class="p">.</span><span class="nf">save</span><span class="p">()</span>
</code></pre></div></div> <p>The consumer.py file plays a critical role in enabling real-time communication with the chatbot. It listens for messages from the frontend, processes them, and returns AI-Generated responses powered by Langflow.</p> <p>Hopefully this post inspired someone to make something similar. I definitely learned a lot about WebSocket connections and how Async technology works, along with some HTMX elements.</p>]]></content><author><name></name></author><category term="django"/><category term="ai"/><category term="python"/><category term="htmx"/><category term="langflow"/><category term="channels"/><summary type="html"><![CDATA[Creating an AI Chatbot using Django]]></summary></entry><entry><title type="html">AWS Landing Zone Accelerator</title><link href="https://www.levitatecold.com/blog/2025/Landing-Zone-Accelerator/" rel="alternate" type="text/html" title="AWS Landing Zone Accelerator"/><published>2025-05-11T17:20:00+00:00</published><updated>2025-05-11T17:20:00+00:00</updated><id>https://www.levitatecold.com/blog/2025/Landing-Zone-Accelerator</id><content type="html" xml:base="https://www.levitatecold.com/blog/2025/Landing-Zone-Accelerator/"><![CDATA[<p>Managing multiple AWS accounts efficiently and securely can be complex. While AWS Control Tower provides a strong foundation with guardrails and automated account setup, it has some limitations when it comes to customization and scaling across enterprise environments.</p> <p>To address this, I used AWS Landing Zone Accelerator (LZA), an open-source solution that builds on Control Tower to provide a more customizable and automated landing zone deployment. This post covers how I set it up, integrated CloudFormation and CodePipeline, and the benefits it brought to my AWS environment.</p> <hr/> <h2 id="why-i-chose-the-aws-lza">Why I chose the AWS LZA?</h2> <p>The LZA brings several enhancements to AWS Control Tower:</p> <ul> <li>Customizable account structure via templates.</li> <li>Automated deployment using CI/CD (CodePipeline).</li> <li>Integration with AWS Security services, IAM, and compliance frameworks.</li> <li>Centralized configuration as code.</li> </ul> <p>This tool was perfect for extending Control Tower to fit my environment’s security, scalability, and automation needs.</p> <hr/> <h2 id="getting-started-initial-setup">Getting Started: Initial Setup</h2> <p>Here is how I bootstrapped the deployment:</p> <ul> <li>Visit the <a href="https://docs.aws.amazon.com/solutions/latest/landing-zone-accelerator-on-aws/prerequisites.html">LZA Implementation Guide</a></li> <li>Go over all the prerequisites.</li> <li>Launch the stack on Step 1.</li> <li>Input all of your parameters (recommend to use S3 to store your configuration files as CodeCommit has ended its lifecycle).</li> <li>Navigate to the AWS CodePipeline console.</li> <li>Insure AWSAccelerator-Installer pipeline is either <code class="language-plaintext highlighter-rouge">In Progress</code> or <code class="language-plaintext highlighter-rouge">Complete</code>.</li> <li>When the installer is complete, the AWSAccelerator-Pipeline is created and should be <code class="language-plaintext highlighter-rouge">In Progress</code>.</li> </ul> <blockquote> <p>Note: This pipleine takes approximately 45 minutes to complete.</p> </blockquote> <hr/> <h2 id="update-the-config-files">Update the config files</h2> <p>Once the pipelines are complete, you are ready to configure.</p> <ul> <li>Navigate to your S3 bucket named aws-accelerator-config-ACCOUNT_ID-REGION.</li> <li>Download the object <code class="language-plaintext highlighter-rouge">zipped/aws-accelerator-config.zip</code> and extract the contents to view your LZA on AWS config files.</li> <li>Each config file is named based on its purpose in the LZA. Sample configs can be found on the offical <a href="https://github.com/awslabs/landing-zone-accelerator-on-aws/tree/main/reference/sample-configurations/lza-sample-config">AWS GitHub repository</a>.</li> <li>When finished editing. Compress the files into a new Zip and upload them back into the same S3 object path.</li> <li>Go to hte CodePipeline console.</li> <li>Select <code class="language-plaintext highlighter-rouge">AWSAccelerator-Pipeline</code>, then <code class="language-plaintext highlighter-rouge">Release Change</code>.</li> </ul> <p>This initiates a new pipeline instantiation and deploys the configuration changes to your environment.</p> <hr/> <h2 id="recap">Recap</h2> <p>Main takeaways I want you to get from this is that CloudFormation (AWS Terraform) sets up:</p> <ul> <li>S3 buckets (for config/code storage)</li> <li>CodePipelines and CodeBuild stages</li> <li>IAM roles for deployment execution</li> <li>And More</li> </ul> <p>Using LZA, I was able to:</p> <ul> <li>Scale Control Tower deployment</li> <li>Enforce consistent governance</li> <li>Automate changes through CI/CD pipelines</li> </ul>]]></content><author><name></name></author><category term="aws"/><category term="lza"/><category term="code-piplines"/><category term="control-tower"/><summary type="html"><![CDATA[Enhancing AWS Control Tower with the AWS Landing Zone Accelerator]]></summary></entry><entry><title type="html">Automating Azure Web App Shutdown When Budget Exceeds a Threshold</title><link href="https://www.levitatecold.com/blog/2025/Automating-Shutdown-Webapp/" rel="alternate" type="text/html" title="Automating Azure Web App Shutdown When Budget Exceeds a Threshold"/><published>2025-02-26T16:23:00+00:00</published><updated>2025-02-26T16:23:00+00:00</updated><id>https://www.levitatecold.com/blog/2025/Automating-Shutdown-Webapp</id><content type="html" xml:base="https://www.levitatecold.com/blog/2025/Automating-Shutdown-Webapp/"><![CDATA[<p>Managing cloud costs is crucial, and Azure provides tools to automate actions when your budget reaches a predefined threshold. In this guide, I’ll walk through how I automated the shutdown of my Azure web app when my budget hit a certain limit.</p> <h2 id="step-1-create-an-automation-runbook">Step 1: Create an Automation Runbook</h2> <p>Azure Automation <strong>Runbooks</strong> allow us to execute scripts automatically. We will create a <strong>Runbook</strong> that stops the web app.</p> <ol> <li>Click the menu (☰) in the Azure Portal.</li> <li>Select <strong>Create a Resource</strong>.</li> <li>Search for <strong>Automation Accounts</strong> and create one.</li> <li>Navigate to the newly created <strong>Automation Account</strong>.</li> <li>On the left panel, select <strong>Process Automation &gt; Runbooks</strong>.</li> <li>Click <strong>Create a Runbook</strong>.</li> <li>Choose <strong>Powershell</strong> as the runtime version and click create.</li> <li>Add the following script to the Runbook.</li> </ol> <div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">param</span><span class="w"> </span><span class="p">(</span><span class="w">
    </span><span class="p">[</span><span class="n">string</span><span class="p">]</span><span class="nv">$ResourceGroupName</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"ResourceGroupName"</span><span class="p">,</span><span class="w">
    </span><span class="p">[</span><span class="n">string</span><span class="p">]</span><span class="nv">$WebAppName</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"WebAppName"</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="n">Connect-AzAccount</span><span class="w"> </span><span class="nt">-Identity</span><span class="w">
</span><span class="n">Stop-AzWebApp</span><span class="w"> </span><span class="nt">-ResourceGroupName</span><span class="w"> </span><span class="nv">$ResourceGroupName</span><span class="w"> </span><span class="nt">-Name</span><span class="w"> </span><span class="nv">$WebAppName</span><span class="w">
</span></code></pre></div></div> <ol> <li>Replace the $ResourceGroupName and WebAppName values with your actual resource details.</li> <li>Click <strong>Publish</strong> to finalize the Runbook.</li> </ol> <h2 id="step-2-create-an-action-group">Step 2: Create an Action Group</h2> <p>An <strong>Action Group</strong> defines what happens when an alert is triggered.</p> <ol> <li>Click the menu (☰) and go to Monitor.</li> <li>Navigate to Alerts.</li> <li>Select <strong>Create Action Group</strong>.</li> <li>Fill in the required information.</li> <li>In the Actions tab: <ul> <li>Set <strong>Action Type</strong> to <strong>Automation Runbook</strong>.</li> <li><strong>Runbook</strong> &gt; Enabled</li> <li><strong>Runbook Source</strong> &gt; User</li> <li>Choose your <strong>subscription</strong>, <strong>Automation account</strong>, and <strong>Runbook</strong>.</li> </ul> </li> <li>Click <strong>OK</strong>, then <strong>Create</strong>.</li> </ol> <h2 id="step-3-set-up-a-budget">Step 3: Set Up a Budget</h2> <p>Now, lets create a budget and configure it to trigger our <strong>Action Group</strong>.</p> <ol> <li>Click the menu (☰) and navigate to <strong>Cost Management + Billing</strong>.</li> <li>Click on <strong>Cost Management &gt; Budgets</strong>.</li> <li>Important: Change the scope to your <strong>Subscription</strong>.</li> <li>Click <strong>Add</strong> and configure the budget according to your needs.</li> <li>In the <strong>Set Alerts</strong> tab, ensure you select your <strong>Action Group</strong>.</li> </ol> <h2 id="step-4-grant-permissions-to-the-automation-account">Step 4: Grant Permissions to the Automation Account</h2> <p>For the <strong>Runbook</strong> to execute properly, it needs permission to manage resources</p> <ol> <li>Go to either your Web App or <strong>Resource Group</strong>.</li> <li>Click <strong>IAM</strong> (Identity and Access Management).</li> <li>Select <strong>Add Role Assignment</strong>.</li> <li>Choose the <strong>Contributor</strong> role.</li> <li>Set Assign Access To as Managed Identity.</li> <li>Select your <strong>Automation Account</strong>.</li> <li>Click <strong>Review and Assign</strong>.</li> </ol> <h2 id="summary">Summary</h2> <p>With this setup:</p> <ol> <li>The budget alert triggers the <strong>Action Group</strong>.</li> <li>The <strong>Action Group</strong> runs the <strong>Automation Runbook</strong>.</li> <li>The <strong>Runbook</strong> executes a script that stops the web app when the budget threshold is met.</li> </ol> <p>This automation ensures your Azure costs stay under control without manual intervention. Hope this guide helps you implement a similar solution!</p>]]></content><author><name></name></author><category term="azure"/><category term="action-groups"/><category term="runbooks"/><category term="automation"/><summary type="html"><![CDATA[Connecting Django App to Azure Postgresql Server]]></summary></entry><entry><title type="html">Azure App Service + PostgreSQL</title><link href="https://www.levitatecold.com/blog/2025/Azure-App-Service-PostgreSQL/" rel="alternate" type="text/html" title="Azure App Service + PostgreSQL"/><published>2025-01-26T16:23:00+00:00</published><updated>2025-01-26T16:23:00+00:00</updated><id>https://www.levitatecold.com/blog/2025/Azure-App-Service-PostgreSQL</id><content type="html" xml:base="https://www.levitatecold.com/blog/2025/Azure-App-Service-PostgreSQL/"><![CDATA[<p>I’m excited to share that I successfully connected my Azure Web App to an Azure Database for PostgreSQL - Flexible Server! This milestone marks a significant step in my development journey, and I want to walk you through the process to help anyone else attempting a similar setup.</p> <h3 id="step-1-create-the-resources">Step 1: Create the Resources</h3> <p>To get started, I created a Django web application and an Azure Database for PostgreSQL - Flexible Server. During the setup, there are a few key options you need to configure:</p> <ol> <li><strong>Add Current Client IP Address</strong>: Ensure this option is selected to include your current IP in the PostgreSQL server’s firewall rules.</li> <li><strong>Allow Public Access</strong>: Enable the option to allow public access to the server using a public IP address.</li> <li><strong>Allow Access from Azure Services</strong>: Select the option to allow public access from any Azure Service within Azure to this server.</li> </ol> <p>Once the PostgreSQL server was created, I navigated to:<br/> <strong>Settings &gt; Service Connector</strong>, and clicked <code class="language-plaintext highlighter-rouge">+Create</code>.</p> <h3 id="service-connector-configuration">Service Connector Configuration</h3> <ul> <li><strong>Service Type</strong>: DB for PostgreSQL Flexible Server</li> <li><strong>Connection Name</strong>: Choose any descriptive name for your connection.</li> <li><strong>Subscription</strong>: Select your Azure subscription.</li> </ul> <p>After creating the Service Connector, I viewed the <strong>Sample Code</strong> tab, which redirected me to the Microsoft Learn documentation. The documentation provides detailed instructions on how to use a connection string to connect your application to the database.</p> <p>Important note: When you created the service connector, Azure already set up environment variables for you in your Azure Web App. To connect to it using django you need to edit your <code class="language-plaintext highlighter-rouge">settings.py</code> and add in those environment variables to the <code class="language-plaintext highlighter-rouge">DATABASES</code> config. For example:</p> <pre><code class="language-python3">host = os.getenv('AZURE_POSTGRESQL_HOST')
user = os.getenv('AZURE_POSTGRESQL_USER')
password = os.getenv('AZURE_POSTGRESQL_PASSWORD')
database = os.getenv('AZURE_POSTGRESQL_NAME')

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': database,
        'USER': user,
        'PASSWORD': password,
        'HOST': host,
        'PORT': '5432',  # Port is 5432 by default 
        'OPTIONS': {'sslmode': 'require'},
    }
}
</code></pre> <blockquote> <p><strong>Pro Tip</strong>: Always follow security best practices by storing sensitive information, such as database credentials, in environment variables instead of hardcoding them into your application.</p> </blockquote> <h2 id="step-2-configure-django">Step 2: Configure Django</h2> <p>Following the Django setup steps in the documentation, I updated my <code class="language-plaintext highlighter-rouge">settings.py</code> file to include the appropriate environment variables for the database connection. Additionally, I installed the required PostgreSQL driver by running:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>psycopg2
</code></pre></div></div> <p>To Test the connection, I pushed all my Django models to the datatbase by running</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">manage</span><span class="p">.</span><span class="n">py</span> <span class="n">makemigrations</span>
<span class="n">python</span> <span class="n">manage</span><span class="p">.</span><span class="n">py</span> <span class="n">migrate</span>
</code></pre></div></div> <p>And just like that… BAM! Pages loaded perfectly, and I was able to perform POST and GET requests without issues.</p> <h3 id="step-3-serve-static-files">Step 3: Serve Static Files</h3> <p>One challenge I encountered was that Gunicorn does not serve static files. For less demanding websites (such as my development environment), I learned that the <strong>WhiteNoise</strong> middleware can be used to serve static files efficiently.</p> <p>To implement WhiteNoise, I ran:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pip</span> <span class="n">install</span> <span class="n">whitenoise</span>
</code></pre></div></div> <p>Next, I updated my <code class="language-plaintext highlighter-rouge">setting.py</code> file to add <code class="language-plaintext highlighter-rouge">whitenoise.middleware.WhiteNoiseMiddleware</code> to the middleware list. It is important to place this middleware <strong>Immediately after</strong> <code class="language-plaintext highlighter-rouge">django.middleware.security.securityMiddleware</code> to ensure it functions correctly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MIDDLEWARE</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">django.middleware.security.SecurityMiddleware</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">whitenoise.middleware.WhiteNoiseMiddleware</span><span class="sh">'</span><span class="p">,</span>
    <span class="c1"># Other middleware...
</span><span class="p">]</span>
</code></pre></div></div> <p>With this setup, I was able to serve static files successfully</p> <h3 id="conclusion">Conclusion</h3> <p>After testing and configuring everything, My development environment was finally in a stable and functional state. This project was a fantastic learning experience, and I am excited to explore more advanced deployments in the future.</p> <p>If you are working on something similar, I hope this guide helps you along the way.</p>]]></content><author><name></name></author><category term="azure"/><category term="database"/><category term="django"/><category term="postgresql"/><summary type="html"><![CDATA[Connecting Django App to Azure Postgresql Server]]></summary></entry><entry><title type="html">Building and Deploying a CI/CD Pipeline for My Azure Web App</title><link href="https://www.levitatecold.com/blog/2025/CI-CD-Pipeline/" rel="alternate" type="text/html" title="Building and Deploying a CI/CD Pipeline for My Azure Web App"/><published>2025-01-18T22:37:00+00:00</published><updated>2025-01-18T22:37:00+00:00</updated><id>https://www.levitatecold.com/blog/2025/CI-CD-Pipeline</id><content type="html" xml:base="https://www.levitatecold.com/blog/2025/CI-CD-Pipeline/"><![CDATA[<p>Today, I accomplished a significant milestone. I created a CI/CD pipeline for my Azure Web App. This project taught me a great deal about <a href="https://docs.github.com/en/actions">GitHub Actions</a>, <a href="https://docs.github.com/en/packages">GitHub Packages</a>, and their role in building and deploying containerized applications. Here I’ll walk you through the process, share the GitHub workflow I used, and explain some key concepts to provide clarity for those interested in DevOps practices.</p> <p>For clarity I deployed a web app using Django. Django is a web framework for Python and is something that I have been learning for a few months now.</p> <h2 id="learning-github-actions-and-github-packages">Learning GitHub Actions and GitHub Packages</h2> <p>GitHub Actions is a powerful tool for automating workflows directly in your GitHub repository. It allows you to define custom workflows using <em>YAML</em> files that are triggered by specific events, such as code pushes or pull requests. With GitHub Actions, you can automate tasks such as building, testing, and deploying applications.</p> <p>GitHub Packages, on the other hand, serves as a container registry where you can store and manage Docker container images. By leveraging GitHub Packages in combination with GitHub Actions, you can automate the entire process of building and deploying your containerized applications.</p> <h2 id="my-github-workflow">My GitHub Workflow</h2> <p>To implement my CI/CD pipeline, I created the following GitHub workflow. It builds a Docker container from my project, pushes the image to GitHub Packages, and deploys it to Azure Web App Services. Here is the workflow:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GitHub recommends pinning actions to a commit SHA.</span>
<span class="c1"># To get a newer version, you will need to update the SHA.</span>
<span class="c1"># You can also reference a tag or branch, but the action may change without warning.</span>

<span class="na">name</span><span class="pi">:</span> <span class="s">Build and deploy container to Azure Web App</span>

<span class="na">env</span><span class="pi">:</span>
  <span class="na">AZURE_WEBAPP_NAME</span><span class="pi">:</span> <span class="s">django-portfolio</span>  <span class="c1"># set this to your application's name</span>

<span class="na">on</span><span class="pi">:</span>
  <span class="na">push</span><span class="pi">:</span>
    <span class="na">branches</span><span class="pi">:</span> <span class="c1"># Trigger action on a push to the main branch</span>
      <span class="pi">-</span> <span class="s">main</span>

<span class="na">permissions</span><span class="pi">:</span>
  <span class="na">contents</span><span class="pi">:</span> <span class="s1">'</span><span class="s">read'</span>
  <span class="na">packages</span><span class="pi">:</span> <span class="s1">'</span><span class="s">write'</span>

<span class="na">jobs</span><span class="pi">:</span>
  <span class="na">build</span><span class="pi">:</span> <span class="c1"># Create a job called build</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>

    <span class="na">steps</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v4</span>

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Set up Docker Buildx</span> <span class="c1"># Name utility</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/setup-buildx-action@6524bf65af31da8d45b59e8c27de4bd072b392f5</span> <span class="c1"># Utility for building and pushing multi-platform Docker Images</span>

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Log in to GitHub container registry</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567</span> <span class="c1"># Logs into GitHub container registry</span>
        <span class="na">with</span><span class="pi">:</span>
          <span class="na">registry</span><span class="pi">:</span> <span class="s">ghcr.io</span>
          <span class="na">username</span><span class="pi">:</span> <span class="s">github username</span>
          <span class="na">password</span><span class="pi">:</span> <span class="s">github secret for github token</span>

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Lowercase the repo name</span>
        <span class="na">run</span><span class="pi">:</span> <span class="s">echo "REPO=${GITHUB_REPOSITORY,,}" &gt;&gt;${GITHUB_ENV}</span> <span class="c1"># Ensure repo name is in lowercase</span>

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Build and push container image to registry</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/build-push-action@b32b51a8eda65d6793cd0494a773d4f6bcef32dc</span> <span class="c1"># Builds Docker image and pushes to GitHub Packages</span>
        <span class="na">with</span><span class="pi">:</span>
          <span class="na">push</span><span class="pi">:</span> <span class="kc">true</span>
          <span class="na">tags</span><span class="pi">:</span> <span class="s">ghcr.io/$:$</span> <span class="c1"># Tags Docker image as repo-name:repo-hash</span>
          <span class="na">file</span><span class="pi">:</span> <span class="s">./Dockerfile</span> <span class="c1"># Builds container based on the Dockerfile in the root of my repo</span>

  <span class="na">deploy</span><span class="pi">:</span> <span class="c1"># Create a job called deploy</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>

    <span class="na">needs</span><span class="pi">:</span> <span class="s">build</span> <span class="c1"># Won't build until build job is complete</span>

    <span class="na">steps</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Lowercase the repo name</span>
        <span class="na">run</span><span class="pi">:</span> <span class="s">echo "REPO=${GITHUB_REPOSITORY,,}" &gt;&gt;${GITHUB_ENV}</span>

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Deploy to Azure Web App</span>
        <span class="na">id</span><span class="pi">:</span> <span class="s">deploy-to-webapp</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">azure/webapps-deploy@2fdd5c3ebb4e540834e86ecc1f6fdcd5539023ee</span> <span class="c1"># Deploys the Docker image from GitHub Packages to Azure Web App</span>
        <span class="na">with</span><span class="pi">:</span>
          <span class="na">app-name</span><span class="pi">:</span> <span class="s">azure web app name</span>
          <span class="na">publish-profile</span><span class="pi">:</span> <span class="s">secret for azure publish-profile</span> <span class="c1"># Store this in GitHub secrets</span>
          <span class="na">images</span><span class="pi">:</span> <span class="s1">'</span><span class="s">ghcr.io/repo:reposha'</span>
</code></pre></div></div> <h2 id="workflow-rundown">WorkFlow Rundown</h2> <ol> <li>Triggers and Permissions: <ul> <li>The workflow is triggered on a push to the main branch.</li> <li>Need read access for contents and write access for packages.</li> </ul> </li> <li>Build Job: <ul> <li><code class="language-plaintext highlighter-rouge">build</code> job starts by checking out the repository</li> <li>Sets up Docker Buildx to build and push multi-platform Docker Images.</li> <li>Logs into GitHub container registry (ghcr.io) using a token.</li> <li>The job ensures the repo name is lowercase and builds the Docker Image using `docker/build-push-action. The image is then pushed to GitHub Packages.</li> </ul> </li> <li>Deploy Job: <ul> <li><code class="language-plaintext highlighter-rouge">deploy</code> job depends on the successful completion of the <code class="language-plaintext highlighter-rouge">build</code> job.</li> <li>It uses the <code class="language-plaintext highlighter-rouge">azure/webapps-deploy</code> action to deploy the Docker image from GitHub packages to Azure Web App Services. The <code class="language-plaintext highlighter-rouge">publish profile</code> is securely stored in GitHub Secrets for authentication.</li> </ul> </li> </ol> <h2 id="setting-up-the-workflow">Setting up the Workflow</h2> <p>To enable this workflow for my project. I had to create a <code class="language-plaintext highlighter-rouge">.github/workflows</code> directory in my project’s root folder. This is where I stored the YAML file for the workflow. Once committed, GitHub automatically recognized and executed the workflow upon a push to the <code class="language-plaintext highlighter-rouge">main</code> branch.</p> <h2 id="reflection">Reflection</h2> <p>This project was an excellent introduction to CI/CD and DevOps practices. I now have a better understanding of:</p> <ul> <li>Automating builds and deployments with GitHub Actions.</li> <li>Managing container images with GitHub Packages.</li> <li>Deploying containerized applications to Azure Web App Services.</li> </ul> <p>By building this pipeline, I have not only streamlined my development but also gained valuable hands-on experience with modern DevOps tools and concepts.</p>]]></content><author><name></name></author><category term="github"/><category term="devops"/><category term="yaml"/><category term="automation"/><summary type="html"><![CDATA[Creating a Markdown to HTML convertor]]></summary></entry><entry><title type="html">Markdown Convertor</title><link href="https://www.levitatecold.com/blog/2024/Markdown-Converter/" rel="alternate" type="text/html" title="Markdown Convertor"/><published>2024-11-27T15:02:00+00:00</published><updated>2024-11-27T15:02:00+00:00</updated><id>https://www.levitatecold.com/blog/2024/Markdown-Converter</id><content type="html" xml:base="https://www.levitatecold.com/blog/2024/Markdown-Converter/"><![CDATA[<h2 id="automating-markdown-to-html-using-python-markdown">Automating Markdown to HTML Using Python-Markdown</h2> <p>As someone who enjoys simplifying workflows, I recently automated a process for one of my Django projects to convert Markdown files into HTML. This is small but effective script saves time and ensures consistency when dealing with Markdown-based content. In this post, I will walk you through the steps from setting up the environment to running the script.</p> <h3 id="why-automate-markdown-conversion">Why automate Markdown Conversion?</h3> <p>Markdown is a lightweight markup language that is easy to write and widely used for documentation, blogs, and other text-based content. However, in web projects like Django, this content often needs to be converted into HTML for rendering in a browser. Automating this conversion eliminates repetitive tasks and ensures a smooth workflow.</p> <hr/> <h3 id="getting-started-setting-up-the-environment">Getting Started: Setting up the Environment</h3> <p>Before writing the script. I set up my project environment using <code class="language-plaintext highlighter-rouge">pipenv</code>. This tool is excellent for managing Python dependencies and keeping environments clean.</p> <ol> <li> <p>Start the virtual Environment:</p> <p>Run the following command to enter the virtual environment:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipenv shell
</code></pre></div> </div> </li> <li> <p>Install the required library:</p> <p>Install the <code class="language-plaintext highlighter-rouge">markdown</code> library, which provides the functionality to convert Markdown to HTML:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipenv <span class="nb">install </span>markdown
</code></pre></div> </div> <p>This automatically updates the <code class="language-plaintext highlighter-rouge">Pipfile.lock</code> to include the library, ensuring dependencies are tracked</p> </li> </ol> <hr/> <h3 id="the-automation-script">The Automation Script</h3> <p>Here is the Python script I created to perform the conversion:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">markdown</span>

<span class="c1"># Using open() to read the Markdown file
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">example.md</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">markdown_content</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="c1"># Convert the Markdown content to HTML and store in a variable
</span><span class="n">html_content</span> <span class="o">=</span> <span class="n">markdown</span><span class="p">.</span><span class="nf">markdown</span><span class="p">(</span><span class="n">markdown_content</span><span class="p">)</span>

<span class="c1"># Write the HTML to the desired file path
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">path/to/store/mdfile/example.html</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">html_content</span><span class="p">)</span>
</code></pre></div></div> <h4 id="how-it-works">How it Works:</h4> <ol> <li> <p>Read the Markdown File:</p> <p>The <code class="language-plaintext highlighter-rouge">open()</code> function reads the <code class="language-plaintext highlighter-rouge">example.md</code> file and stores its content in <code class="language-plaintext highlighter-rouge">markdown_content</code>.</p> </li> <li> <p>Convert Markdown to HTML:</p> <p>Using the <code class="language-plaintext highlighter-rouge">markdown.markdown()</code> function, the Markdown content is converted into an HTML string.</p> </li> <li> <p>Write the HTML to a File:</p> <p>The script writes the resulting HTML into a file at the specified path <code class="language-plaintext highlighter-rouge">(path/to/store/mdfile/example.html)</code>.</p> </li> </ol> <p>This process is straightforward, yet it can be a game-changer in projects that involve dynamic content updates or static page generation.</p> <hr/> <h3 id="executing-and-results">Executing and Results</h3> <p>Running the script is as simple as executing:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python script_name.py <span class="c"># if using windows</span>

or

python3 script_name.py <span class="c"># if using linux</span>
</code></pre></div></div> <p>Once the script runs, the converted HTML file is created in the directory path in the specified directory. You can open it in a browser to verify the results.</p>]]></content><author><name></name></author><category term="python"/><category term="django"/><category term="markdown"/><category term="automation"/><summary type="html"><![CDATA[Creating a Markdown to HTML convertor]]></summary></entry><entry><title type="html">Dockerizing Django Application</title><link href="https://www.levitatecold.com/blog/2024/Dockerizing-Django/" rel="alternate" type="text/html" title="Dockerizing Django Application"/><published>2024-11-03T15:38:00+00:00</published><updated>2024-11-03T15:38:00+00:00</updated><id>https://www.levitatecold.com/blog/2024/Dockerizing-Django</id><content type="html" xml:base="https://www.levitatecold.com/blog/2024/Dockerizing-Django/"><![CDATA[<p>Starting a new project is always exciting, especially when it brings together technologies like Django and Docker. For this project, I decided to build a to-do application using Django as the web framework. To simplify development and ensure consistency across environments, I set up a Docker container and a Docker Compose file. This setup allows for seamless deployment, making it as simple as running a single command to launch the app.</p> <p>In this blog, I’ll guide you through creating the Dockerfile and using Docker Compose to automate container setup, along with a special Nginx configuration to enhance the deployment. As someone aspiring to enter DevOps, I believe it’s crucial to understand the fundamentals of web frameworks. Exploring concepts like virtual environments, pipfiles, piplocks, environment variables, and configuring web servers like Nginx has been both educational and a lot of fun.</p> <p>If you’re new to Django, I highly recommend following a tutorial on building a web app with it. This not only broadens your Python skills but also introduces you to HTML, CSS, and other tools that can make an application visually engaging.</p> <hr/> <h2 id="create-a-django-application">Create a Django Application</h2> <p>As I mentioned earlier, diving into Django is a great way to get comfortable with its functionality and structure. To start, I worked through the official Django documentation’s <a href="https://docs.djangoproject.com/en/5.1/">polling application tutorial</a>, which helped me understand the basics. From there, I chose to build a simple application of my own —to-do list app— as a foundation to practice and experiment with new features.</p> <p>After some research, I decided to follow the <a href="https://realpython.com/django-todo-lists/">Real Python tutorial on creating a Django to-do list</a>. If you’re interested in learning alongside me, I encourage you to try building this application first. Then, come back here to see how we can containerize it with Docker and deploy it locally on your home network.</p> <hr/> <h2 id="creating-a-dockerfile">Creating a Dockerfile</h2> <p>A Dockerfile is a script containing a series of instructions that tells Docker how to build a custom container image. You specify things like the base image (e.g., Python or Ubuntu), the application code, required libraries, and any setup commands needed to get your app up and running. When you build this file, Docker uses these instructions to create a container image that you can deploy consistently across different environments.</p> <h3 id="the-dockerfile">The DockerFile</h3> <p>Here’s a basic Dockerfile for a Django application:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># First we import the python image from DockerHub (https://hub.docker.com/_/python)</span>

<span class="s">FROM python:3.12</span>

<span class="c1"># Standard environment variables when creating a python based dockerfile</span>
<span class="c1"># First env tells python not to write .pyc files. These files are unnecessary for a container.</span>
<span class="c1"># Second env tells python to output directly into the console without buffering</span>

<span class="s">ENV PYTHONDONTWRITEBYTECODE=1</span>
<span class="s">ENV PYTHONUNBUFFERED=1</span> 

<span class="c1"># Make a directory called app to store your django project, update any packages, and update pip</span>

<span class="s">RUN mkdir /app &amp;&amp; \</span> 
    <span class="s">apt update &amp;&amp; \</span>
    <span class="s">pip install --upgrade pip</span>

<span class="c1"># Set the new directory as the working directory    </span>

<span class="s">WORKDIR /app</span>

<span class="c1"># Copy all the files in your current directory on your host to the current directory of the container</span>

<span class="s">COPY . .</span>

<span class="c1"># Run commands to install the requirements from your requirements.txt</span>

<span class="s">RUN pip install -r requirements.txt</span>

<span class="c1"># Expose port 8000</span>

<span class="s">EXPOSE </span><span class="m">8000</span>

<span class="c1"># Commands to run when container is finished. Seperate each string with a comma.</span>

<span class="s">CMD ["gunicorn", "-b", "0.0.0.0:8000", "todo_website.wsgi:application"]</span>
</code></pre></div></div> <h3 id="why-we-use-gunicorn">Why we use Gunicorn</h3> <p>By default Django’s development server, <code class="language-plaintext highlighter-rouge">python manage.py runserver</code>, is intended for development and is not optimized for handling multiple requests at scale or ensuring stability needed in production.</p> <p>Gunicorn is bridge between Django and the web, efficiently handling multiple incoming requests by spawning multiple worker processes that manage these requests concurrently. This makes the app more responsive and capable of handling higher traffic volumes.</p> <p>In a typical setup, Gunicorn is often used together with a web server like Nginx. Nginx handles static file serving, load balancing, and reverse proxying to Gunicorn, which then processes the actual application requests. This combination helps ensure performance, stability, and security for Django applications in production environments</p> <p>Here is an example of a Nginx config for Gunicorn:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># nginx.conf</span>

<span class="s">server {</span>
    <span class="s">listen 80;</span>

    <span class="s"># Replace this with your domain or IP</span>
    <span class="s">server_name example.domain.com;</span>

    <span class="s"># Serve static files</span>
    <span class="s">location /static/ {</span> <span class="c1"># This block tells nginx to handle requests that start with /static/</span>
        <span class="s">alias /app/static/;</span> <span class="c1"># This sets /app/static/ as the directory where nginx should look for the files requested under /static/</span>
    <span class="err">}</span>

    <span class="c1"># Proxy requests to Gunicorn</span>
    <span class="s">location / {</span>
        <span class="s">proxy_pass http://web:8000;</span> <span class="c1"># web is based on the service name in the docker compose file</span>
        <span class="s">proxy_set_header Host $host;</span> 
        <span class="s">proxy_set_header X-Real-IP $remote_addr;</span>
        <span class="s">proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span>
        <span class="s">proxy_set_header X-Forwarded-Proto $scheme;</span>
    <span class="s">}</span>
<span class="err">}</span>
</code></pre></div></div> <hr/> <h2 id="creating-the-docker-compose">Creating the Docker Compose</h2> <p>Docker Compose is a tool that simplifies the process of managing multi-container Docker applications. With Docker Compose, you can define all the services, networks, and volumes your app needs in a single <code class="language-plaintext highlighter-rouge">docker-compose.yaml</code> file. This will specify how each container should be configured, such as which Docker image to use, any environment variables, and how containers should interact.</p> <p>Here is an example of my <code class="language-plaintext highlighter-rouge">docker-compose.yaml</code> file:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">name</span><span class="pi">:</span> <span class="s">todoapp</span>

<span class="na">services</span><span class="pi">:</span>
  <span class="na">web</span><span class="pi">:</span> <span class="c1"># service name</span>
    <span class="na">build</span><span class="pi">:</span> <span class="c1"># This tells Docker to generate the image directly from the project's Dockerfile instead of using a pre-built one like we did for nginx.</span>
      <span class="na">context</span><span class="pi">:</span> <span class="s">.</span> <span class="c1"># Defines a path to a directory that contains a Dockerfile, or a URL to a git repo.</span>
      <span class="na">dockerfile</span><span class="pi">:</span> <span class="s">Dockerfile</span> <span class="c1"># Can set an alternate Dockerfile. So this file could be called "app.Dockerfile".</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">static_volume:/app/static/</span> <span class="c1"># Stores our static files</span>
      <span class="pi">-</span> <span class="s">./db.sqlite3:/app/db.sqlite3</span> <span class="c1"># Mount the SQLite database file if you want to persist it.</span>
    <span class="na">expose</span><span class="pi">:</span> <span class="c1"># Expose is only used to connect to other services in the Dockerfile</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">8000"</span>
    
  <span class="na">nginx</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:latest</span>
    <span class="na">ports</span><span class="pi">:</span> <span class="c1"># Ports is used if you want clients/services outside of the Dockerfile to connect.</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">80:80"</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">static_volume:/app/static</span> <span class="c1"># Link the same static files directory</span>
      <span class="pi">-</span> <span class="s">./nginx.conf:/etc/nginx/conf.d/default.conf</span> <span class="c1"># Replace the contents of nginx.conf into the default.conf of nginx.</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">web</span> <span class="c1"># Nginx should depend on the web service, it gets built after the web service.</span>

<span class="na">volumes</span><span class="pi">:</span>
  <span class="na">static_volume</span><span class="pi">:</span> <span class="c1"># Create the static_volume</span>
</code></pre></div></div> <p>Here you can see I have two services. The first service image is the DockerFile I created in my apps root directory. The second service is a Nginx image pulled from DockerHub. With this setup, when I run <code class="language-plaintext highlighter-rouge">docker compose up</code>, Docker will build both these containers and set up how they should connect to each other. This makes it much easier to deploy and manage complex applications.</p>]]></content><author><name></name></author><category term="python"/><category term="docker-compose"/><category term="django"/><category term="docker"/><category term="nginx"/><summary type="html"><![CDATA[Using docker compose to containerize my Django application]]></summary></entry><entry><title type="html">InfluxDB and Grafana</title><link href="https://www.levitatecold.com/blog/2024/Grafana-Dashboard/" rel="alternate" type="text/html" title="InfluxDB and Grafana"/><published>2024-09-15T16:23:00+00:00</published><updated>2024-09-15T16:23:00+00:00</updated><id>https://www.levitatecold.com/blog/2024/Grafana-Dashboard</id><content type="html" xml:base="https://www.levitatecold.com/blog/2024/Grafana-Dashboard/"><![CDATA[<p>In managing a homelab, keeping a eye on my server’s performance is crucial for maintaining stability and optimizing resource usage. As I expanded my Proxmox environment, I wanted a better way to monitor the health of my server in real-time. The provided monitoring for my server given by proxmox itself was very limited and felt lacking. However, it was a nice feature given that the purpose of Proxmox as a whole isn’t just for monitoring. After researching various options, I decided to set up a monitoring solution using <strong>InfluxDB</strong> and <strong>Grafana</strong>. InfluxDB is a high performance time-series database, perfect for storing metrics, while Grafana provides rich, customizable visualizations.</p> <p>In this post, I will discuss with you how I integrated these tools to monitor my Proxmox server. From setting up InfluxDB to visualizing metrics in Grafana.</p> <hr/> <h2 id="creating-the-docker-compose">Creating the docker compose</h2> <p>After reading various documents, reddit post, videos, etc. I came to a conclusion that I would create docker containers for both Grafana and InfluxDB. Given that both have verified publishers for their containers on <a href="https://hub.docker.com/">Docker Hub</a> I felt comfortable moving forward with my decision.</p> <p>Step 1:<br/> Create a directory to store the compose file.<br/></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir </span>monitoring
</code></pre></div></div> <p>Use the <code class="language-plaintext highlighter-rouge">ls</code> command to verify the directory was made.</p> <p>Step 2:<br/> Create the compose file.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">touch </span>compose.yaml
</code></pre></div></div> <p>Step 3:<br/> Use your favorite editor to edit the compose file.</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">services</span><span class="pi">:</span>
  <span class="na">grafana</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">grafana/grafana</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">grafana</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">3089:3089</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">monitoring_network</span> <span class="c1"># mount network</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./conf:/usr/share/grafana/conf</span> <span class="c1"># mount server "conf" file to the container "conf" file, whatever changes are made to the server "conf" file are changed on the container "conf" file when restarted</span>
      <span class="pi">-</span> <span class="s">grafana-volume:/var/lib/grafana</span> <span class="c1"># mount grafana-volume</span>

  <span class="na">influxdb</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">influxdb</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">influxdb</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">8086:8086</span>
      <span class="pi">-</span> <span class="s">8089:8089/udp</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">monitoring_network</span> <span class="c1">#mount network</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">influxdb-volume-config:/etc/influxdb2</span>
      <span class="pi">-</span> <span class="s">influxdb-volume:/var/lib/influxdb2</span> <span class="c1"># mount influxdb-volume</span>

<span class="na">networks</span><span class="pi">:</span>
  <span class="na">monitoring_network</span><span class="pi">:</span>
    <span class="na">external</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># Use this when the network is already created using &lt;docker network create "name"&gt;</span>

<span class="na">volumes</span><span class="pi">:</span>
  <span class="na">grafana-volume</span><span class="pi">:</span>
    <span class="na">external</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># Use this when the volume is already created using &lt;docker volume create "name"&gt;</span>
  <span class="na">influxdb-volume</span><span class="pi">:</span>
    <span class="na">external</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">influxdb-volume-config</span><span class="pi">:</span>
    <span class="na">external</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div> <blockquote> <p>Please note that the <code class="language-plaintext highlighter-rouge">./conf</code> volume on the Grafana service was necessary for me because I needed to change the default port of 3000 to 3089 inside the <code class="language-plaintext highlighter-rouge">default.ini</code> file of the container. So if you do not need to change the port, use the recommended port option of 3000 for the Grafana service and remove the <code class="language-plaintext highlighter-rouge">./conf</code> volume.</p> </blockquote> <hr/> <h2 id="create-your-volume-and-network">Create your volume and network</h2> <p>In our compose file we set the network and volumes to <code class="language-plaintext highlighter-rouge">external: true</code> meaning we have to create these resources ourselves or else the compose file will error out because it cannot find the volume or network.</p> <p>Run these commands to create our volumes and network:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker volume create grafana-volume
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker volume create influxdb-volume
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker volume create influxdb-volume-config
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker network create monitoring_network
</code></pre></div></div> <p>You can use <code class="language-plaintext highlighter-rouge">docker volume ls</code> and <code class="language-plaintext highlighter-rouge">docker network ls</code> to list out your current networks/volumes you have on your environment.</p> <hr/> <h2 id="testing-your-creations">Testing your creations!</h2> <p>Run <code class="language-plaintext highlighter-rouge">docker compose up -d</code> to run docker in detach mode and wait a few minutes.</p> <p>Run <code class="language-plaintext highlighter-rouge">docker ps -a</code> to list all of your docker containers and you can view the status of the containers you just created.</p> <p>Navigate to your containers. <code class="language-plaintext highlighter-rouge">&lt;serverip:8086&gt;</code> for InfluxDB and <code class="language-plaintext highlighter-rouge">&lt;serverip:3000&gt;</code> for Grafana. Unless you changed your ports like I did, then you should be good to go. I added all my services to my <strong>Homepage</strong> dashboard for easier navigation between all my services. I will make a post going over what Homepage is in a later post, but for now if you are interested, here is a GitHub link to <a href="https://github.com/gethomepage/homepage">HomePage</a>. Once inside your containers. Make sure you create a account in InfluxDB and the default username and password for Grafana is admin:admin. Once you are in your instances I highly recommend watching Christian Lempa’s video <a href="https://www.youtube.com/watch?v=f2eyVfCTLi0&amp;t=403s">My new Proxmox Monitoring Tools: InfluxDB2 + Grafana</a> on what to do next. He goes over what I went through and MORE!</p> <hr/> <h2 id="what-should-i-do-now">What should I do now?</h2> <p>Just like you I was confused on how everything worked and what to do next. When watching Christian Lempa’s video, I imported the <a href="https://grafana.com/grafana/dashboards/15356-proxmox-cluster-flux/">Proxmox Cluster [Flux]</a> dashboard from the <a href="https://grafana.com/">Grafana website</a>, but I felt as if I wasn’t learning anything from doing that, even though the dashboard is great and amazing and provides me with everything I want from a monitoring standpoint. My next objective in this learning journey is to create my own dashboard, learn a little bit of flux, and see how everything pans out.</p>]]></content><author><name></name></author><category term="monitoring"/><category term="database"/><category term="flux"/><category term="grafana"/><category term="influxdb"/><category term="walkthrough"/><summary type="html"><![CDATA[Using flux and grafana to monitor my Proxmox server]]></summary></entry><entry><title type="html">Hosting my first website</title><link href="https://www.levitatecold.com/blog/2024/Jekyll-Website/" rel="alternate" type="text/html" title="Hosting my first website"/><published>2024-08-11T16:10:00+00:00</published><updated>2024-08-11T16:10:00+00:00</updated><id>https://www.levitatecold.com/blog/2024/Jekyll-Website</id><content type="html" xml:base="https://www.levitatecold.com/blog/2024/Jekyll-Website/"><![CDATA[<p>When I first decided to create my own personal website, I was faced with a common dilemma: which platform should I use? The obvious choice seemed to be website builders like Wix, Squarespace, and Weebly given there simplicity and popularity. However, as I explored my options, I found myself drawn to something more customizable and developer-friendly…. <strong>Jekyll</strong>.</p> <hr/> <h2 id="why-jekyll">Why Jekyll?</h2> <p>One of the main reasons to I chose Jekyll is the unparalleled flexibility it offers. Unlike Wordpress, which can sometimes feel restrictive with its themes and plugins, Jekyll gives me complete control over every aspect of my website. From the layout and design to the functionality and content structure, I can tailor anything to match my vision if I wanted to do so. This level of customization was exactly what I was looking for as someone who enjoys tinkering and learning new things with code and experimenting with different ideas.</p> <hr/> <h2 id="integration-with-github-pages">Integration with GitHub Pages</h2> <p>Another significant advantage of Jekyll is its seamless integration with <strong>Github Pages</strong>. As someone who loves exploring projects on GitHub, I was excited to use a platform that works so well with Git’s version control system. With Jekyll, I can host my website directly from a GitHub repository, making it easy to manage updates, track changes, and collaborate with others if needed. Plus, deploying the site is as simple as pushing commits to my repository—no need to worry about complex setups or hosting fees.</p> <hr/> <h2 id="the-appeal-of-using-wsl-and-git">The Appeal of Using WSL and Git</h2> <p>I wanted to learn more about using <strong>Windows Subsystem for Linux</strong> (WSL) and <strong>Git</strong>. With Jekyll, I could easily set up my development environment on WSL, clone my repository using Git, and manage my site entirely from the command line. So not am I only getting more hands on with using Git commands, I am learning how to traverse through a linux environment right on my windows system, thanks to WSL! This hands-on approach allowed me to deepen my understanding of version control and site deployment, skills that I wanted to develop as a someone who is eager to learn new things in IT.</p> <hr/> <h2 id="performance-and-security">Performance and Security</h2> <p>Jekyll’s static site generation also stood out to me for its performance and security benefits. Jekyll generates static HTML files so that my website loads faster and is less vulnerable to attacks compared to a dynamic WordPress site that relies on a database. This means I can focus on creating more content and less on dealing with potential security issues or performance bottlenecks.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>In the end, Jekyll was the clear winner for me because it provided the perfect balance of flexibility, control, and learning potential. It is a platform that aligned with my passion for coding and understanding CI/CD pipelines, as well as heightening other skills. If you are looking for a powerful and developer-friendly way to build a website, I highly recommend giving Jekyll a try. Thank you for taking the time to read my story, and I look forward to sharing more about my experiences and giving you all tutorials on how I manage to accomplish everything you see here and in future posts!</p> <hr/> <h2 id="links">Links</h2> <p>I’d also like to give a special shoutout to <a href="https://github.com/alshedivat">Reddit user alshedivat</a> and their 209 contributors for creating the fantastic <a href="https://github.com/alshedivat/al-folio">ai-folio template</a>, which has been incredibly helpful in my Jekyll journey. If you’re interested in learning more about Jekyll, be sure to check out the official <a href="https://jekyllrb.com/">Jekyll website</a>. And for anyone looking for high-quality themes, I highly recommend exploring the <a href="https://github.com/topics/jekyll-theme">jekyll-theme repository on GitHub</a>.</p>]]></content><author><name></name></author><category term="Jekyll"/><category term="hosting"/><category term="website"/><summary type="html"><![CDATA[Learning how to set up and host my own website using Jekyll.]]></summary></entry></feed>